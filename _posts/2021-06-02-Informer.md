---
title: "[논문 겉핥기] Informer"
strapline: "Deep learning"
description: "장기 시계열 예측을 위한 인포머"
header:
 overlay_image: /assets/images/triangular.jpeg
categories:
  - "paper"
tag:
  - "machine learning"
  - "deep learning"
  - "time series"
toc: true
last_modified_at: 2021-06-02
comments: true
mathjax: true
---

# Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting

**https://arxiv.org/abs/2012.07436**

인포머는 트랜스포머 구조를 바탕으로 장기 시계열 예측 (Long sequence time-series forecasting, LSTF)을 효과적으로 수행하기 위한 딥러닝 모델이다.

## 1. Backgrounds
LSTF를 효과적으로 수행하기 위해서는  **(a) extra ordinary long-range alignment ability**와  **(b) efficient operations on long sequence inputs and output**이 필요하다. 트랜스포머는 (a)를 수행함에 있어서 탁월한 능력을 보이지만 (b) 측면에서 약점을 가지고 있다. 
이러한 약점을 다시 요약하면 다음과 같다.

1. The quadratic computation of self-attention. >> 어텐션 스코어 연산에서 수행되는 dot product 텀에 의해 메모리와 시간이 $\mathcal{O}(L^2)$로 소요.
2. The memory bottlenect in stacking layers ofr long inputs. >> 1번 항목의 quadratic term이 레이어 수에 따라 비례하여 증가.
3. The speed plunge in predicting long outputs. >> 스텝바이스텝으로 incremental하게 forecasting을 수행하여 엄청나게 긴 시퀀스를 예측할 때 오래걸림.

기존 논문들은 1번 문제를 해결하는데 중점을 두는데, 인포머는 1,2,3의 제약사항을 모두 포괄한다.

## 2. How?

1. ProbSparse self-attention을 제안하여 $\mathcal{O}(L^2)$를 $\mathcal{O}(L\text{log}L)로 줄임.
2. Self-attention distilling을 제안하여 J개의 레이어에 사용되는 메모리를 $\mathcal{O}((2-\epsilon)L log L)$로 줄임.
3. Generative style decoder를 제안하여 one-forward step으로 전체 시퀀스를 예측함.

## 3. Ideas
### 3.1 ProbSparse self-attention

기존 dot product attention을 수행하는 경우 query, key, value matrix $(Q,K,V)$에 대해 row-wise로 dot product를 계산하기때문에 computation과 memory가 row의 수에 따라 $\mathcal{O}(L_Q L_K)$가 필요함.
그런데 특정 쿼리는 특정 키에 반응하는 것이 자연스럽기 때문에 이러한 attention mechanism 상에서 sparse한 성질을 고려하는 것이 효과적임.
그래서 sparsity를 갖는 쿼리들을 selective하게 사용할 것이고 query의 sparsity를 측정하기 위한 Query sparsity measurement $M(q_i,K)$을 제안함. 즉 M값이 높은 애들은 attention score가 sparse하게 분포되어있다는 것임. 
Top $u=c\cdot \text{ln}L_Q$ 개의 query만을 사용함.
그러나 여전히 M 계산 과정에서 row-wise로 계산해야하기 때문에 $\mathcal{O}(L_Q L_K)$가 필요함. 
이에 대한 empirical approximation으로 $\bar{M}$을 제안하며 $\bar{M}$은 u개의 쿼리를 랜덤하게 샘플링하여 sparsity measure를 연산하기때문에 마찬가지로 $\mathcal{O}(L_K\text{ln}L_Q)$ complexity가 감소함.

### 3.2 Self-attention distilling

트랜스포머에서는 입력 시퀀스의 길이가 어텐션 레이어를 통과하는 과정에서 유지됨. Self-attention distilling은 1-d conv를 통해 인접 타임스텝의 정보를 추출하고 이후 max-pooling을 사용해 레이어 통과시 시퀀스의 길이가 1/2로 감소함.

### 3.3 Generative inference
트랜스포머에서는 디코더에 시작 토큰을 입력하고 스텝바이스텝으로 다음 타임스텝의 값을 예측하고 붙여나감.
여기서는 디코더에 입력시퀀스를 예측 전 시퀀스의 정보 일부와 예측하고자 하는 타임스텝들의 시간적 정보를 인코딩한 값을 concatenation해서 하나의 입력 시퀀스 벡터를 만듬.
디코더는 이러한 입력 시퀀스 벡터로부터 한번에 전체 시퀀스에 대한 예측을 수행함.
